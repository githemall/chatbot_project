import os
import traceback
import streamlit as st
from langchain_huggingface import HuggingFaceEndpoint
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

from langchain_huggingface import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


# --- 1. RAG ì²´ì¸(Chain) ì„¤ì • ---

# (1) LLM ëª¨ë¸ ë¡œë“œ (Hugging Face ë¡œì»¬ íŒŒì´í”„ë¼ì¸)

@st.cache_resource # Streamlitì˜ ìºì‹œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
def get_llm_pipeline():
    print("--- [ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹œì‘] ---")
    model_id = "google/gemma-2b-it"
    
    # 1. í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª‡ ë¶„ ì†Œìš”)
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    
    # 2. Transformers íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipe = pipeline(
        "text-generation", # (Phi-3ì™€ ë™ì¼í•œ íƒ€ì… ì‚¬ìš©)
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,
        eos_token_id=tokenizer.eos_token_id
    )
    
    # 3. LangChainì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë˜í•‘(Wrapping)í•©ë‹ˆë‹¤.
    llm = HuggingFacePipeline(pipeline=pipe)
    print("--- [ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ] ---")
    return llm

# llm ë³€ìˆ˜ì— ë¡œë“œëœ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ì„ í• ë‹¹í•©ë‹ˆë‹¤.
llm = get_llm_pipeline()
# --------------------------------------------------


# (2) ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë³€ê²½ ì—†ìŒ - ì´ë¯¸ ë¡œì»¬ì…ë‹ˆë‹¤)
@st.cache_resource # ì„ë² ë”© ëª¨ë¸ë„ ìºì‹œ ì²˜ë¦¬
def get_embeddings():
    model_name = "jhgan/ko-sbert-nli"
    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': True}
    return HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
embeddings = get_embeddings()


# (3) ë²¡í„° DB ë¡œë“œ (ë³€ê²½ ì—†ìŒ)
persist_directory = './chroma_db' # (ì´ì „ì— setup_db.pyë¡œ ìƒì„±í•œ DB)
vectorstore = Chroma(
    persist_directory=persist_directory,
    embedding_function=embeddings
)

# (4. 5. 6. RAG ì²´ì¸ ë° í”„ë¡¬í”„íŠ¸ - ë³€ê²½ ì—†ìŒ)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

template = """
Answer the following 'Question' based *only* on the 'Context' provided.
If the information is not in the context, say "I don't have that information."

Context:
{context}

Question:
{question}

Answer:
"""
prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {
        "context": lambda x: retriever.invoke(x["question"]),
        "question": lambda x: x["question"]
    }
    | prompt
    | llm
    | StrOutputParser()
)


st.title("ğŸ’¬ ë™ì•„ë¦¬ ê·œì • ì•ˆë‚´ ì±—ë´‡ (Local Ver.)")


if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if user_input := st.chat_input("ë™ì•„ë¦¬ë°© ì‚¬ìš© ì‹œê°„ì„ ì•Œë ¤ì¤˜"):
    
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # --- [ 2. (í•µì‹¬) ë””ë²„ê¹… ë¸”ë¡ ì¶”ê°€! ] ---
    # ì±—ë´‡ì´ ë‹µë³€í•˜ê¸° ì „ì—, retrieverê°€ ë¬´ì—‡ì„ ì°¾ëŠ”ì§€ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
    st.subheader("--- [ë””ë²„ê¹…: ê²€ìƒ‰ê¸°(Retriever) ê²°ê³¼] ---")
    try:
        # (retrieverëŠ” íŒŒì¼ ìƒë‹¨ì—ì„œ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŒ)
        retrieved_docs = retriever.invoke(user_input)
        
        if retrieved_docs:
            st.write(f"âœ… {len(retrieved_docs)}ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            for i, doc in enumerate(retrieved_docs):
                st.write(f"--- [ë¬¸ì„œ {i+1}] ---")
                # ì°¾ì€ ë¬¸ì„œì˜ ë‚´ìš© ì¼ë¶€ë¥¼ í‘œì‹œ
                st.write(f"ë‚´ìš©: {doc.page_content[:200]}...")
                # ì°¾ì€ ë¬¸ì„œì˜ ì¶œì²˜(metadata)ë¥¼ í‘œì‹œ
                st.write(f"ì¶œì²˜: {doc.metadata}")
        else:
            st.error("âŒ 'retriever.invoke'ê°€ ì•„ë¬´ ë¬¸ì„œë„ ë°˜í™˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            st.error("ì´ê²ƒì´ 'I don't have...'ì˜ ì›ì¸ì…ë‹ˆë‹¤.")

    except Exception as e_debug:
        st.error(f"âŒ 'retriever.invoke' í˜¸ì¶œ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e_debug}")
    st.write("--- [ë””ë²„ê¹… ì¢…ë£Œ] ---")
    # -----------------------------------------------

    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # RAG ì²´ì¸ ì‹¤í–‰
                response = rag_chain.invoke({"question": user_input})
                st.markdown(response)
                # 3. ì±—ë´‡ ë©”ì‹œì§€ ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": response})

            except Exception as e:
                # (1) í„°ë¯¸ë„ì— ê°•ì œë¡œ ì „ì²´ ì˜¤ë¥˜ ë‚´ìš©(Traceback)ì„ ì¸ì‡„í•©ë‹ˆë‹¤.
                print("!!! ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ !!!")
                traceback.print_exc() 
                print("!!!!!!!!!!!!!!!!!!!!!!!!!!!")
                
                # (2) ì›¹ UIì—ë„ ì˜¤ë¥˜ ë©”ì‹œì§€ 'e'ì˜ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ í‘œì‹œí•©ë‹ˆë‹¤.
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")